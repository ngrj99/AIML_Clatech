{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a540a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52f55057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and load a simulation environment\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0a1a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment: CartPole\n",
    "# Documentation Link: https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
    "#\n",
    "# Goal: Is to balance the pole on the cart by moving the cart left or right for a given episode\n",
    "#\n",
    "# Agent: Cart\n",
    "#\n",
    "# Actions: 0 ---- Left\n",
    "#          1 ---- Right\n",
    "#\n",
    "# State: [\"Cart Position\",\"Cart Velocity\",\"Pole Angle\",\"poleVelocity\"]\n",
    "#\n",
    "# Reward: 1 for every step taken such that the pole is balanced successfully.\n",
    "#\n",
    "# Termination Condition:\n",
    "# 1. Pole Angle is greater than +-12 DEGREE\n",
    "# 2. Cart Position is greater than +-2.4\n",
    "# 3. Episode length greater than 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcdb210",
   "metadata": {},
   "source": [
    "# Common Functions used in Gymnasium Env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7735a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b37515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to get the initial state?\n",
    "#To bring the agent to the start of the env\n",
    "\n",
    "observation,info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765dbaca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01543725, -0.0290997 ,  0.03162589,  0.0208516 ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation\n",
    "# State: [\"Cart Position\",\"Cart Velocity\",\"Pole Angle\",\"poleVelocity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c82a05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32a29a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How to generate random action?\n",
    "action = env.action_space.sample()\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "862c3ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.01601925,  0.16555476,  0.03204293, -0.26168764], dtype=float32),\n",
       " 1.0,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How to submit action to the environment\n",
    "\n",
    "observation, reward, terminatedStatus, truncatedStatus, info = env.step(action)\n",
    "\n",
    "#observation --- state (current position of the agent)\n",
    "#reward -------- The outcome achieved by the agent based on the current action (+1 or -1)\n",
    "#terminatedStatus -- Whether termination condition is satisified or not (Binary outcome)\n",
    "# truncatedStatus -- Whether episodes are complete or not\n",
    "#info --- additional relevant info of the environment\n",
    "\n",
    "observation, reward, terminatedStatus, truncatedStatus, info "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e80622",
   "metadata": {},
   "source": [
    "# Run a Single Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee9a9119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Step 0 Given Action 1 I got reward 1.0 and next state [-0.01839833  0.23026982 -0.00155256 -0.25890252]\n",
      "Episode Step 1 Given Action 0 I got reward 1.0 and next state [-0.01379293  0.03517007 -0.00673061  0.03329031]\n",
      "Episode Step 2 Given Action 1 I got reward 1.0 and next state [-0.01308953  0.2303879  -0.0060648  -0.26150858]\n",
      "Episode Step 3 Given Action 1 I got reward 1.0 and next state [-0.00848177  0.42559588 -0.01129497 -0.5560982 ]\n",
      "Episode Step 4 Given Action 1 I got reward 1.0 and next state [ 3.0144049e-05  6.2087458e-01 -2.2416936e-02 -8.5231823e-01]\n",
      "Episode Step 5 Given Action 0 I got reward 1.0 and next state [ 0.01244764  0.4260653  -0.0394633  -0.56676775]\n",
      "Episode Step 6 Given Action 0 I got reward 1.0 and next state [ 0.02096894  0.2315185  -0.05079865 -0.28677395]\n",
      "Episode Step 7 Given Action 0 I got reward 1.0 and next state [ 0.02559931  0.03715643 -0.05653413 -0.0105353 ]\n",
      "Episode Step 8 Given Action 1 I got reward 1.0 and next state [ 0.02634244  0.23304166 -0.05674484 -0.32050592]\n",
      "Episode Step 9 Given Action 0 I got reward 1.0 and next state [ 0.03100327  0.03877181 -0.06315496 -0.0462442 ]\n"
     ]
    }
   ],
   "source": [
    "#initialize the state\n",
    "observation,info = env.reset()\n",
    "\n",
    "for episodeStep in range(10):\n",
    "    #Choose a random action\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    #Supply action to the env\n",
    "    newState,reward,isTerminated,isTruncated,info = env.step(action)\n",
    "\n",
    "    #Print info\n",
    "    print(f\"Episode Step {episodeStep} Given Action {action} I got reward {reward} and next state {newState}\")\n",
    "\n",
    "    #Check for Termination\n",
    "    if isTerminated:\n",
    "        print(\"GAME OVER --- Terminated!!!\")\n",
    "        env.close()\n",
    "        break\n",
    "\n",
    "#Check for Truncation(Episode ended)\n",
    "if isTruncated:\n",
    "    print(\"Episode Over. Total Allowed Steps Done. Agent was able to balance pole successfully :)\")\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ed5f4e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24649e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Step 0 Given Action 0 I got reward 1.0 and next state [-0.00258496 -0.22707945  0.03346527  0.32298362]\n",
      "Episode Step 1 Given Action 1 I got reward 1.0 and next state [-0.00712655 -0.03244961  0.03992495  0.0410393 ]\n",
      "Episode Step 2 Given Action 1 I got reward 1.0 and next state [-0.00777555  0.16207775  0.04074573 -0.23878442]\n",
      "Episode Step 3 Given Action 1 I got reward 1.0 and next state [-0.00453399  0.35659465  0.03597004 -0.51834166]\n",
      "Episode Step 4 Given Action 0 I got reward 1.0 and next state [ 0.0025979   0.16098522  0.02560321 -0.21454439]\n",
      "Episode Step 5 Given Action 0 I got reward 1.0 and next state [ 0.00581761 -0.03449324  0.02131232  0.08610372]\n",
      "Episode Step 6 Given Action 0 I got reward 1.0 and next state [ 0.00512774 -0.2299141   0.0230344   0.3854338 ]\n",
      "Episode Step 7 Given Action 0 I got reward 1.0 and next state [ 5.2945939e-04 -4.2535535e-01  3.0743072e-02  6.8528944e-01]\n",
      "Episode Step 8 Given Action 1 I got reward 1.0 and next state [-0.00797765 -0.2306734   0.04444886  0.4024415 ]\n",
      "Episode Step 9 Given Action 1 I got reward 1.0 and next state [-0.01259112 -0.03620915  0.05249769  0.1240969 ]\n",
      "Episode Step 10 Given Action 0 I got reward 1.0 and next state [-0.0133153  -0.23204237  0.05497963  0.43286926]\n",
      "Episode Step 11 Given Action 0 I got reward 1.0 and next state [-0.01795615 -0.4278979   0.06363701  0.7423645 ]\n",
      "Episode Step 12 Given Action 1 I got reward 1.0 and next state [-0.0265141  -0.23370941  0.0784843   0.47036803]\n",
      "Episode Step 13 Given Action 1 I got reward 1.0 and next state [-0.03118829 -0.03977872  0.08789166  0.20341818]\n",
      "Episode Step 14 Given Action 1 I got reward 1.0 and next state [-0.03198387  0.15398352  0.09196003 -0.0602964 ]\n",
      "Episode Step 15 Given Action 0 I got reward 1.0 and next state [-0.0289042  -0.04232839  0.0907541   0.2599254 ]\n",
      "Episode Step 16 Given Action 0 I got reward 1.0 and next state [-0.02975076 -0.23862082  0.09595261  0.5797974 ]\n",
      "Episode Step 17 Given Action 0 I got reward 1.0 and next state [-0.03452318 -0.43494716  0.10754856  0.9010991 ]\n",
      "Episode Step 18 Given Action 1 I got reward 1.0 and next state [-0.04322213 -0.24143386  0.12557054  0.6440646 ]\n",
      "Episode Step 19 Given Action 0 I got reward 1.0 and next state [-0.0480508  -0.43806145  0.13845183  0.97350454]\n",
      "Episode Step 20 Given Action 0 I got reward 1.0 and next state [-0.05681203 -0.634742    0.15792193  1.3062772 ]\n",
      "Episode Step 21 Given Action 1 I got reward 1.0 and next state [-0.06950687 -0.44193456  0.18404746  1.0669012 ]\n",
      "Episode Step 22 Given Action 0 I got reward 1.0 and next state [-0.07834556 -0.63895077  0.20538549  1.4112394 ]\n",
      "Episode Step 23 Given Action 1 I got reward 1.0 and next state [-0.09112458 -0.44688046  0.23361027  1.1891489 ]\n",
      "GAME OVER --- Terminated!!!\n",
      "Episode Step 0 Given Action 1 I got reward 1.0 and next state [ 0.0011274   0.20507514 -0.01259442 -0.24992533]\n",
      "Episode Step 1 Given Action 1 I got reward 1.0 and next state [ 0.00522891  0.40037468 -0.01759293 -0.546554  ]\n",
      "Episode Step 2 Given Action 0 I got reward 1.0 and next state [ 0.0132364   0.20550427 -0.02852401 -0.25946572]\n",
      "Episode Step 3 Given Action 0 I got reward 1.0 and next state [ 0.01734649  0.01080088 -0.03371333  0.02408563]\n",
      "Episode Step 4 Given Action 0 I got reward 1.0 and next state [ 0.0175625  -0.18382178 -0.03323161  0.30594388]\n",
      "Episode Step 5 Given Action 1 I got reward 1.0 and next state [ 0.01388607  0.01175758 -0.02711274  0.00296853]\n",
      "Episode Step 6 Given Action 1 I got reward 1.0 and next state [ 0.01412122  0.20725766 -0.02705336 -0.29814398]\n",
      "Episode Step 7 Given Action 0 I got reward 1.0 and next state [ 0.01826637  0.01253159 -0.03301624 -0.01411447]\n",
      "Episode Step 8 Given Action 0 I got reward 1.0 and next state [ 0.018517   -0.1821017  -0.03329853  0.26797134]\n",
      "Episode Step 9 Given Action 1 I got reward 1.0 and next state [ 0.01487497  0.01347926 -0.02793911 -0.03502543]\n",
      "Episode Step 10 Given Action 0 I got reward 1.0 and next state [ 0.01514456 -0.18123113 -0.02863961  0.24871314]\n",
      "Episode Step 11 Given Action 0 I got reward 1.0 and next state [ 0.01151993 -0.3759326  -0.02366535  0.5322267 ]\n",
      "Episode Step 12 Given Action 0 I got reward 1.0 and next state [ 0.00400128 -0.5707139  -0.01302082  0.8173598 ]\n",
      "Episode Step 13 Given Action 0 I got reward 1.0 and next state [-0.007413   -0.76565516  0.00332638  1.105919  ]\n",
      "Episode Step 14 Given Action 1 I got reward 1.0 and next state [-0.0227261  -0.5705771   0.02544476  0.8142814 ]\n",
      "Episode Step 15 Given Action 0 I got reward 1.0 and next state [-0.03413764 -0.7660381   0.04173039  1.1148579 ]\n",
      "Episode Step 16 Given Action 0 I got reward 1.0 and next state [-0.0494584  -0.9616823   0.06402755  1.420334  ]\n",
      "Episode Step 17 Given Action 0 I got reward 1.0 and next state [-0.06869205 -1.1575354   0.09243423  1.7323229 ]\n",
      "Episode Step 18 Given Action 1 I got reward 1.0 and next state [-0.09184276 -0.9635822   0.12708068  1.4697721 ]\n",
      "Episode Step 19 Given Action 1 I got reward 1.0 and next state [-0.11111441 -0.7702234   0.15647613  1.2193339 ]\n",
      "Episode Step 20 Given Action 0 I got reward 1.0 and next state [-0.12651888 -0.9669773   0.1808628   1.5566756 ]\n",
      "Episode Step 21 Given Action 0 I got reward 1.0 and next state [-0.14585842 -1.1637458   0.21199632  1.8998982 ]\n",
      "GAME OVER --- Terminated!!!\n",
      "Episode Step 0 Given Action 1 I got reward 1.0 and next state [-0.04738218  0.19203928  0.03551452 -0.30078885]\n",
      "Episode Step 1 Given Action 1 I got reward 1.0 and next state [-0.04354139  0.3866375   0.02949874 -0.5820631 ]\n",
      "Episode Step 2 Given Action 1 I got reward 1.0 and next state [-0.03580864  0.581334    0.01785748 -0.86530936]\n",
      "Episode Step 3 Given Action 0 I got reward 1.0 and next state [-2.4181962e-02  3.8597360e-01  5.5129133e-04 -5.6706572e-01]\n",
      "Episode Step 4 Given Action 0 I got reward 1.0 and next state [-0.01646249  0.19084392 -0.01079002 -0.27420914]\n",
      "Episode Step 5 Given Action 0 I got reward 1.0 and next state [-0.01264561 -0.00412242 -0.0162742   0.01505116]\n",
      "Episode Step 6 Given Action 1 I got reward 1.0 and next state [-0.01272806  0.19122909 -0.01597318 -0.28272173]\n",
      "Episode Step 7 Given Action 1 I got reward 1.0 and next state [-0.00890348  0.3865752  -0.02162762 -0.5803994 ]\n",
      "Episode Step 8 Given Action 1 I got reward 1.0 and next state [-0.00117197  0.5819934  -0.03323561 -0.87981623]\n",
      "Episode Step 9 Given Action 0 I got reward 1.0 and next state [ 0.01046789  0.3873384  -0.05083193 -0.5977644 ]\n",
      "Episode Step 10 Given Action 0 I got reward 1.0 and next state [ 0.01821466  0.1929632  -0.06278722 -0.32151636]\n",
      "Episode Step 11 Given Action 1 I got reward 1.0 and next state [ 0.02207393  0.3889205  -0.06921754 -0.6333204 ]\n",
      "Episode Step 12 Given Action 0 I got reward 1.0 and next state [ 0.02985233  0.1948289  -0.08188395 -0.3632139 ]\n",
      "Episode Step 13 Given Action 1 I got reward 1.0 and next state [ 0.03374891  0.39101338 -0.08914823 -0.6805516 ]\n",
      "Episode Step 14 Given Action 1 I got reward 1.0 and next state [ 0.04156918  0.58725303 -0.10275926 -0.99991703]\n",
      "Episode Step 15 Given Action 1 I got reward 1.0 and next state [ 0.05331424  0.78358716 -0.1227576  -1.3230227 ]\n",
      "Episode Step 16 Given Action 0 I got reward 1.0 and next state [ 0.06898598  0.59021115 -0.14921805 -1.0711417 ]\n",
      "Episode Step 17 Given Action 0 I got reward 1.0 and next state [ 0.08079021  0.39734304 -0.17064089 -0.8287619 ]\n",
      "Episode Step 18 Given Action 1 I got reward 1.0 and next state [ 0.08873707  0.59433556 -0.18721613 -1.1698843 ]\n",
      "Episode Step 19 Given Action 0 I got reward 1.0 and next state [ 0.10062378  0.40207508 -0.21061382 -0.9412535 ]\n",
      "GAME OVER --- Terminated!!!\n",
      "Episode Step 0 Given Action 0 I got reward 1.0 and next state [ 0.01688062 -0.22875057  0.04389013  0.3536754 ]\n",
      "Episode Step 1 Given Action 0 I got reward 1.0 and next state [ 0.01230561 -0.42446822  0.05096364  0.6598687 ]\n",
      "Episode Step 2 Given Action 0 I got reward 1.0 and next state [ 0.00381624 -0.620261    0.06416102  0.9681534 ]\n",
      "Episode Step 3 Given Action 0 I got reward 1.0 and next state [-0.00858898 -0.81618303  0.08352409  1.2802821 ]\n",
      "Episode Step 4 Given Action 0 I got reward 1.0 and next state [-0.02491264 -1.012264    0.10912973  1.5979058 ]\n",
      "Episode Step 5 Given Action 0 I got reward 1.0 and next state [-0.04515792 -1.208497    0.14108784  1.9225248 ]\n",
      "Episode Step 6 Given Action 0 I got reward 1.0 and next state [-0.06932785 -1.4048245   0.17953834  2.2554321 ]\n",
      "Episode Step 7 Given Action 1 I got reward 1.0 and next state [-0.09742434 -1.2117866   0.22464699  2.0230308 ]\n",
      "GAME OVER --- Terminated!!!\n",
      "Episode Step 0 Given Action 0 I got reward 1.0 and next state [-0.03777338 -0.17848311  0.01091666  0.2761502 ]\n",
      "Episode Step 1 Given Action 0 I got reward 1.0 and next state [-0.04134304 -0.3737591   0.01643966  0.57225615]\n",
      "Episode Step 2 Given Action 1 I got reward 1.0 and next state [-0.04881822 -0.17887147  0.02788479  0.28479725]\n",
      "Episode Step 3 Given Action 0 I got reward 1.0 and next state [-0.05239565 -0.37437978  0.03358073  0.5861428 ]\n",
      "Episode Step 4 Given Action 1 I got reward 1.0 and next state [-0.05988325 -0.17974389  0.04530359  0.3042244 ]\n",
      "Episode Step 5 Given Action 0 I got reward 1.0 and next state [-0.06347813 -0.3754812   0.05138807  0.61084384]\n",
      "Episode Step 6 Given Action 1 I got reward 1.0 and next state [-0.07098775 -0.18111375  0.06360495  0.334779  ]\n",
      "Episode Step 7 Given Action 1 I got reward 1.0 and next state [-0.07461002  0.01304802  0.07030053  0.0628125 ]\n",
      "Episode Step 8 Given Action 1 I got reward 1.0 and next state [-0.07434907  0.2070953   0.07155678 -0.20688811]\n",
      "Episode Step 9 Given Action 0 I got reward 1.0 and next state [-0.07020716  0.01102693  0.06741901  0.10748154]\n",
      "Episode Step 10 Given Action 0 I got reward 1.0 and next state [-0.06998662 -0.18499313  0.06956865  0.42064983]\n",
      "Episode Step 11 Given Action 1 I got reward 1.0 and next state [-0.07368648  0.00907771  0.07798164  0.15068443]\n",
      "Episode Step 12 Given Action 1 I got reward 1.0 and next state [-0.07350493  0.20300142  0.08099534 -0.11641376]\n",
      "Episode Step 13 Given Action 1 I got reward 1.0 and next state [-0.0694449   0.39687508  0.07866706 -0.38248426]\n",
      "Episode Step 14 Given Action 1 I got reward 1.0 and next state [-0.0615074   0.59079707  0.07101738 -0.64936334]\n",
      "Episode Step 15 Given Action 0 I got reward 1.0 and next state [-0.04969146  0.39476144  0.05803011 -0.3351896 ]\n",
      "Episode Step 16 Given Action 0 I got reward 1.0 and next state [-0.04179623  0.1988637   0.05132632 -0.02478633]\n",
      "Episode Step 17 Given Action 1 I got reward 1.0 and next state [-0.03781895  0.39321345  0.05083059 -0.30084375]\n",
      "Episode Step 18 Given Action 1 I got reward 1.0 and next state [-0.02995469  0.58757544  0.04481371 -0.5770724 ]\n",
      "Episode Step 19 Given Action 0 I got reward 1.0 and next state [-0.01820318  0.39185494  0.03327227 -0.2706156 ]\n",
      "Episode Step 20 Given Action 1 I got reward 1.0 and next state [-0.01036608  0.5864867   0.02785995 -0.55262136]\n",
      "Episode Step 21 Given Action 0 I got reward 1.0 and next state [ 0.00136366  0.3909848   0.01680752 -0.2512926 ]\n",
      "Episode Step 22 Given Action 0 I got reward 1.0 and next state [0.00918335 0.19562693 0.01178167 0.04664402]\n",
      "Episode Step 23 Given Action 0 I got reward 1.0 and next state [1.3095890e-02 3.3803002e-04 1.2714554e-02 3.4302077e-01]\n",
      "Episode Step 24 Given Action 1 I got reward 1.0 and next state [0.01310265 0.19527681 0.01957497 0.05437421]\n",
      "Episode Step 25 Given Action 1 I got reward 1.0 and next state [ 0.01700819  0.3901127   0.02066245 -0.23206896]\n",
      "Episode Step 26 Given Action 0 I got reward 1.0 and next state [0.02481044 0.19470169 0.01602107 0.06705932]\n",
      "Episode Step 27 Given Action 1 I got reward 1.0 and next state [ 0.02870447  0.38959032  0.01736226 -0.22052611]\n",
      "Episode Step 28 Given Action 0 I got reward 1.0 and next state [0.03649628 0.19422455 0.01295174 0.07758261]\n",
      "Episode Step 29 Given Action 0 I got reward 1.0 and next state [ 0.04038077 -0.00108065  0.01450339  0.37432355]\n",
      "Episode Step 30 Given Action 0 I got reward 1.0 and next state [ 0.04035916 -0.19640559  0.02198986  0.67154396]\n",
      "Episode Step 31 Given Action 1 I got reward 1.0 and next state [ 0.03643105 -0.00159612  0.03542074  0.38586494]\n",
      "Episode Step 32 Given Action 0 I got reward 1.0 and next state [ 0.03639913 -0.19720253  0.04313804  0.68950206]\n",
      "Episode Step 33 Given Action 1 I got reward 1.0 and next state [ 0.03245508 -0.00270494  0.05692808  0.41070572]\n",
      "Episode Step 34 Given Action 1 I got reward 1.0 and next state [0.03240098 0.19156568 0.06514219 0.13649966]\n",
      "Episode Step 35 Given Action 0 I got reward 1.0 and next state [ 0.03623229 -0.00442591  0.06787219  0.44900176]\n",
      "Episode Step 36 Given Action 1 I got reward 1.0 and next state [0.03614377 0.18967366 0.07685222 0.17846186]\n",
      "Episode Step 37 Given Action 0 I got reward 1.0 and next state [ 0.03993724 -0.00645915  0.08042146  0.494365  ]\n",
      "Episode Step 38 Given Action 0 I got reward 1.0 and next state [ 0.03980806 -0.20261772  0.09030876  0.8112703 ]\n",
      "Episode Step 39 Given Action 1 I got reward 1.0 and next state [ 0.03575571 -0.00884131  0.10653417  0.54830486]\n",
      "Episode Step 40 Given Action 0 I got reward 1.0 and next state [ 0.03557888 -0.20528586  0.11750027  0.87256294]\n",
      "Episode Step 41 Given Action 0 I got reward 1.0 and next state [ 0.03147316 -0.4017928   0.13495152  1.1997565 ]\n",
      "Episode Step 42 Given Action 0 I got reward 1.0 and next state [ 0.02343731 -0.5983774   0.15894665  1.5315078 ]\n",
      "Episode Step 43 Given Action 1 I got reward 1.0 and next state [ 0.01146976 -0.4054882   0.1895768   1.292355  ]\n",
      "Episode Step 44 Given Action 0 I got reward 1.0 and next state [ 0.00335999 -0.6024454   0.21542391  1.6379002 ]\n",
      "GAME OVER --- Terminated!!!\n",
      "Episode Step 0 Given Action 0 I got reward 1.0 and next state [-0.01253142 -0.20149997  0.02133339  0.26679975]\n",
      "Episode Step 1 Given Action 0 I got reward 1.0 and next state [-0.01656142 -0.3969198   0.02666938  0.56613433]\n",
      "Episode Step 2 Given Action 0 I got reward 1.0 and next state [-0.02449981 -0.5924055   0.03799207  0.8670985 ]\n",
      "Episode Step 3 Given Action 1 I got reward 1.0 and next state [-0.03634793 -0.3978206   0.05533404  0.5865987 ]\n",
      "Episode Step 4 Given Action 1 I got reward 1.0 and next state [-0.04430434 -0.20351551  0.06706601  0.3118471 ]\n",
      "Episode Step 5 Given Action 0 I got reward 1.0 and next state [-0.04837465 -0.39952564  0.07330295  0.6249039 ]\n",
      "Episode Step 6 Given Action 0 I got reward 1.0 and next state [-0.05636516 -0.59559023  0.08580104  0.93974286]\n",
      "Episode Step 7 Given Action 0 I got reward 1.0 and next state [-0.06827696 -0.79175746  0.10459589  1.2581058 ]\n",
      "Episode Step 8 Given Action 0 I got reward 1.0 and next state [-0.08411212 -0.98805076  0.129758    1.5816317 ]\n",
      "Episode Step 9 Given Action 1 I got reward 1.0 and next state [-0.10387313 -0.79468995  0.16139065  1.3320707 ]\n",
      "Episode Step 10 Given Action 0 I got reward 1.0 and next state [-0.11976693 -0.9914366   0.18803206  1.6705987 ]\n",
      "Episode Step 11 Given Action 0 I got reward 1.0 and next state [-0.13959566 -1.1881801   0.22144403  2.0154686 ]\n",
      "GAME OVER --- Terminated!!!\n",
      "Episode Step 0 Given Action 1 I got reward 1.0 and next state [-0.0271677   0.17435127  0.00577994 -0.33855268]\n",
      "Episode Step 1 Given Action 0 I got reward 1.0 and next state [-0.02368068 -0.02085245 -0.00099111 -0.04405271]\n",
      "Episode Step 2 Given Action 1 I got reward 1.0 and next state [-0.02409772  0.1742837  -0.00187216 -0.33704817]\n",
      "Episode Step 3 Given Action 0 I got reward 1.0 and next state [-0.02061205 -0.02081156 -0.00861313 -0.04495622]\n",
      "Episode Step 4 Given Action 1 I got reward 1.0 and next state [-0.02102828  0.17443283 -0.00951225 -0.34034416]\n",
      "Episode Step 5 Given Action 1 I got reward 1.0 and next state [-0.01753962  0.36968884 -0.01631914 -0.6360115 ]\n",
      "Episode Step 6 Given Action 1 I got reward 1.0 and next state [-0.01014585  0.5650345  -0.02903936 -0.9337886 ]\n",
      "Episode Step 7 Given Action 1 I got reward 1.0 and next state [ 1.1548424e-03  7.6053596e-01 -4.7715135e-02 -1.2354535e+00]\n",
      "Episode Step 8 Given Action 1 I got reward 1.0 and next state [ 0.01636556  0.95623755 -0.0724242  -1.5426947 ]\n",
      "Episode Step 9 Given Action 0 I got reward 1.0 and next state [ 0.03549031  0.76205707 -0.1032781  -1.2734616 ]\n",
      "Episode Step 10 Given Action 1 I got reward 1.0 and next state [ 0.05073145  0.9583339  -0.12874733 -1.5966179 ]\n",
      "Episode Step 11 Given Action 1 I got reward 1.0 and next state [ 0.06989813  1.1547258  -0.1606797  -1.9265147 ]\n",
      "Episode Step 12 Given Action 0 I got reward 1.0 and next state [ 0.09299265  0.961651   -0.19920999 -1.6876701 ]\n",
      "Episode Step 13 Given Action 0 I got reward 1.0 and next state [ 0.11222567  0.76931226 -0.23296338 -1.4630488 ]\n",
      "GAME OVER --- Terminated!!!\n",
      "Episode Step 0 Given Action 1 I got reward 1.0 and next state [ 0.01321124  0.21451256  0.02669769 -0.31533328]\n",
      "Episode Step 1 Given Action 1 I got reward 1.0 and next state [ 0.01750149  0.40924424  0.02039102 -0.5994785 ]\n",
      "Episode Step 2 Given Action 1 I got reward 1.0 and next state [ 0.02568638  0.6040751   0.00840145 -0.8856694 ]\n",
      "Episode Step 3 Given Action 1 I got reward 1.0 and next state [ 0.03776788  0.7990819  -0.00931193 -1.1756995 ]\n",
      "Episode Step 4 Given Action 1 I got reward 1.0 and next state [ 0.05374952  0.9943236  -0.03282592 -1.471287  ]\n",
      "Episode Step 5 Given Action 1 I got reward 1.0 and next state [ 0.07363599  1.1898313  -0.06225166 -1.7740395 ]\n",
      "Episode Step 6 Given Action 1 I got reward 1.0 and next state [ 0.09743262  1.3855972  -0.09773245 -2.0854099 ]\n",
      "Episode Step 7 Given Action 0 I got reward 1.0 and next state [ 0.12514456  1.1915892  -0.13944066 -1.8244741 ]\n",
      "Episode Step 8 Given Action 0 I got reward 1.0 and next state [ 0.14897634  0.9982631  -0.17593013 -1.5781624 ]\n",
      "Episode Step 9 Given Action 1 I got reward 1.0 and next state [ 0.1689416   1.1949902  -0.20749338 -1.9201552 ]\n",
      "Episode Step 10 Given Action 1 I got reward 1.0 and next state [ 0.19284141  1.3916515  -0.24589649 -2.2693858 ]\n",
      "GAME OVER --- Terminated!!!\n",
      "Episode Step 0 Given Action 0 I got reward 1.0 and next state [ 0.01835794 -0.198207    0.02787594  0.26222995]\n",
      "Episode Step 1 Given Action 0 I got reward 1.0 and next state [ 0.0143938  -0.39371556  0.03312054  0.5635733 ]\n",
      "Episode Step 2 Given Action 0 I got reward 1.0 and next state [ 0.00651949 -0.5892862   0.044392    0.8665041 ]\n",
      "Episode Step 3 Given Action 0 I got reward 1.0 and next state [-0.00526624 -0.7849833   0.06172208  1.1728075 ]\n",
      "Episode Step 4 Given Action 1 I got reward 1.0 and next state [-0.0209659  -0.5907155   0.08517823  0.9000955 ]\n",
      "Episode Step 5 Given Action 0 I got reward 1.0 and next state [-0.03278021 -0.78688204  0.10318014  1.2182906 ]\n",
      "Episode Step 6 Given Action 1 I got reward 1.0 and next state [-0.04851785 -0.59323055  0.12754595  0.9596394 ]\n",
      "Episode Step 7 Given Action 0 I got reward 1.0 and next state [-0.06038246 -0.7898149   0.14673874  1.2895175 ]\n",
      "Episode Step 8 Given Action 0 I got reward 1.0 and next state [-0.07617876 -0.9864664   0.17252909  1.6243113 ]\n",
      "Episode Step 9 Given Action 1 I got reward 1.0 and next state [-0.09590809 -0.79374367  0.20501532  1.3899913 ]\n",
      "Episode Step 10 Given Action 1 I got reward 1.0 and next state [-0.11178296 -0.60167897  0.23281515  1.1677808 ]\n",
      "GAME OVER --- Terminated!!!\n",
      "Episode Step 0 Given Action 1 I got reward 1.0 and next state [ 0.04343272  0.18455848  0.01231136 -0.2521237 ]\n",
      "Episode Step 1 Given Action 0 I got reward 1.0 and next state [ 0.04712389 -0.01073708  0.00726889  0.04441689]\n",
      "Episode Step 2 Given Action 0 I got reward 1.0 and next state [ 0.04690915 -0.20596251  0.00815723  0.33938432]\n",
      "Episode Step 3 Given Action 1 I got reward 1.0 and next state [ 0.0427899  -0.01095757  0.01494491  0.04928485]\n",
      "Episode Step 4 Given Action 1 I got reward 1.0 and next state [ 0.04257075  0.18394694  0.01593061 -0.23864561]\n",
      "Episode Step 5 Given Action 1 I got reward 1.0 and next state [ 0.04624969  0.37883773  0.0111577  -0.5262613 ]\n",
      "Episode Step 6 Given Action 1 I got reward 1.0 and next state [ 5.3826444e-02  5.7380092e-01  6.3247274e-04 -8.1540757e-01]\n",
      "Episode Step 7 Given Action 0 I got reward 1.0 and next state [ 0.06530246  0.3786703  -0.01567568 -0.5225258 ]\n",
      "Episode Step 8 Given Action 0 I got reward 1.0 and next state [ 0.07287586  0.18377246 -0.02612619 -0.23482339]\n",
      "Episode Step 9 Given Action 1 I got reward 1.0 and next state [ 0.07655132  0.37925777 -0.03082266 -0.5356315 ]\n",
      "Episode Step 10 Given Action 0 I got reward 1.0 and next state [ 0.08413647  0.18458247 -0.04153529 -0.2528177 ]\n",
      "Episode Step 11 Given Action 1 I got reward 1.0 and next state [ 0.08782812  0.38027215 -0.04659165 -0.5583069 ]\n",
      "Episode Step 12 Given Action 0 I got reward 1.0 and next state [ 0.09543356  0.18583411 -0.05775778 -0.28065935]\n",
      "Episode Step 13 Given Action 0 I got reward 1.0 and next state [ 0.09915025 -0.00841842 -0.06337097 -0.00673778]\n",
      "Episode Step 14 Given Action 1 I got reward 1.0 and next state [ 0.09898188  0.18755242 -0.06350572 -0.3187226 ]\n",
      "Episode Step 15 Given Action 0 I got reward 1.0 and next state [ 0.10273293 -0.00661029 -0.06988018 -0.04672376]\n",
      "Episode Step 16 Given Action 1 I got reward 1.0 and next state [ 0.10260072  0.1894405  -0.07081465 -0.36061028]\n",
      "Episode Step 17 Given Action 1 I got reward 1.0 and next state [ 0.10638953  0.38549384 -0.07802686 -0.67475533]\n",
      "Episode Step 18 Given Action 0 I got reward 1.0 and next state [ 0.11409941  0.19153799 -0.09152196 -0.40762338]\n",
      "Episode Step 19 Given Action 0 I got reward 1.0 and next state [ 0.11793017 -0.00217514 -0.09967443 -0.14513968]\n",
      "Episode Step 20 Given Action 0 I got reward 1.0 and next state [ 0.11788666 -0.19573887 -0.10257722  0.11450902]\n",
      "Episode Step 21 Given Action 0 I got reward 1.0 and next state [ 0.11397189 -0.38925284 -0.10028705  0.37314937]\n",
      "Episode Step 22 Given Action 0 I got reward 1.0 and next state [ 0.10618683 -0.5828178  -0.09282406  0.63260293]\n",
      "Episode Step 23 Given Action 1 I got reward 1.0 and next state [ 0.09453048 -0.38653183 -0.080172    0.3121904 ]\n",
      "Episode Step 24 Given Action 0 I got reward 1.0 and next state [ 0.08679984 -0.58042556 -0.07392819  0.5785515 ]\n",
      "Episode Step 25 Given Action 0 I got reward 1.0 and next state [ 0.07519133 -0.7744378  -0.06235716  0.84705985]\n",
      "Episode Step 26 Given Action 1 I got reward 1.0 and next state [ 0.05970257 -0.57852316 -0.04541597  0.5354379 ]\n",
      "Episode Step 27 Given Action 1 I got reward 1.0 and next state [ 0.04813211 -0.38279298 -0.03470721  0.22879767]\n",
      "Episode Step 28 Given Action 0 I got reward 1.0 and next state [ 0.04047625 -0.5774022  -0.03013126  0.51033384]\n",
      "Episode Step 29 Given Action 1 I got reward 1.0 and next state [ 0.02892821 -0.38186902 -0.01992458  0.20830996]\n",
      "Episode Step 30 Given Action 1 I got reward 1.0 and next state [ 0.02129083 -0.18646792 -0.01575838 -0.09059095]\n",
      "Episode Step 31 Given Action 1 I got reward 1.0 and next state [ 0.01756147  0.00887632 -0.0175702  -0.3882037 ]\n",
      "Episode Step 32 Given Action 0 I got reward 1.0 and next state [ 0.01773899 -0.18599188 -0.02533427 -0.10111189]\n",
      "Episode Step 33 Given Action 0 I got reward 1.0 and next state [ 0.01401916 -0.38074178 -0.02735651  0.18347171]\n",
      "Episode Step 34 Given Action 0 I got reward 1.0 and next state [ 0.00640432 -0.5754618  -0.02368708  0.4674007 ]\n",
      "Episode Step 35 Given Action 0 I got reward 1.0 and next state [-0.00510492 -0.77024126 -0.01433906  0.7525245 ]\n",
      "Episode Step 36 Given Action 0 I got reward 1.0 and next state [-2.0509740e-02 -9.6516258e-01  7.1142695e-04  1.0406609e+00]\n",
      "Episode Step 37 Given Action 0 I got reward 1.0 and next state [-0.03981299 -1.1602939   0.02152465  1.3335671 ]\n",
      "Episode Step 38 Given Action 1 I got reward 1.0 and next state [-0.06301887 -0.96544987  0.04819599  1.0476965 ]\n",
      "Episode Step 39 Given Action 0 I got reward 1.0 and next state [-0.08232787 -1.1611772   0.06914992  1.3551105 ]\n",
      "Episode Step 40 Given Action 1 I got reward 1.0 and next state [-0.10555141 -0.9669878   0.09625213  1.0848365 ]\n",
      "Episode Step 41 Given Action 0 I got reward 1.0 and next state [-0.12489117 -1.1632386   0.11794885  1.4061047 ]\n",
      "Episode Step 42 Given Action 1 I got reward 1.0 and next state [-0.14815594 -0.96976167  0.14607094  1.1525022 ]\n",
      "Episode Step 43 Given Action 1 I got reward 1.0 and next state [-0.16755117 -0.7768154   0.169121    0.90895724]\n",
      "Episode Step 44 Given Action 0 I got reward 1.0 and next state [-0.18308748 -0.9737728   0.18730015  1.2496634 ]\n",
      "Episode Step 45 Given Action 0 I got reward 1.0 and next state [-0.20256294 -1.1707351   0.2122934   1.5946846 ]\n",
      "GAME OVER --- Terminated!!!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "\n",
    "for episodeCount in range(1,11):\n",
    "    #initialize the state\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "    observation,info = env.reset()\n",
    "\n",
    "    for episodeStep in range(400):\n",
    "        #Choose a random action\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        #Supply action to the env\n",
    "        newState,reward,isTerminated,isTruncated,info = env.step(action)\n",
    "\n",
    "        #Add small delay and call render to see game in execution\n",
    "        time.sleep(0.02) #20ms delay\n",
    "        env.render()\n",
    "\n",
    "        #Print info\n",
    "        print(f\"Episode Step {episodeStep} Given Action {action} I got reward {reward} and next state {newState}\")\n",
    "\n",
    "        #Check for Termination\n",
    "        if isTerminated:\n",
    "            print(\"GAME OVER --- Terminated!!!\")\n",
    "            env.close()\n",
    "            break\n",
    "\n",
    "    #Check for Truncation(Episode ended)\n",
    "    if isTruncated:\n",
    "        (\"Episode Over. Total Allowed Steps Done. Agent was able to balance pole successfully :)\")\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80c22ff",
   "metadata": {},
   "source": [
    "# Run Multiple Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eba9df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
