{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "734ce28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------ Configuration ------------------\n",
    "# You can change these defaults as needed.\n",
    "\n",
    "# Model to use — assignment requests GPT-3.5 Turbo, but you may use a newer model if you wish.\n",
    "# Keep it configurable:\n",
    "#OPENAI_CHAT_MODEL = os.getenv(\"OPENAI_CHAT_MODEL\", \"gpt-3.5-turbo\")\n",
    "\n",
    "# Chroma persistence dir (so the index can be reused without rebuilding)\n",
    "CHROMA_DIR = Path(\".Dataset/the_nestle_hr_policy_pdf_2012.pdf\")\n",
    "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Chunking parameters (tune for your documents)\n",
    "CHUNK_SIZE = 500\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "# Retriever params\n",
    "SEARCH_K = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32ab7299",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_knowledge_base(pdf_paths, reset_db=False):\n",
    "    '''\n",
    "    Create (or refresh) a Chroma vector store from a list of PDF file paths.\n",
    "    Returns a retriever you can plug into RetrievalQA.\n",
    "    '''\n",
    "    if reset_db and CHROMA_DIR.exists():\n",
    "        shutil.rmtree(CHROMA_DIR)\n",
    "        CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) Load PDFs\n",
    "    docs = []\n",
    "    for p in pdf_paths:\n",
    "        loader = PyPDFLoader(str(p))\n",
    "        docs.extend(loader.load())\n",
    "\n",
    "    # 2) Split into chunks\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP, separators=['\\n\\n', '\\n', ' ', '']\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    #  Visualize\n",
    "    for i, chunk in enumerate(chunks[:3]):\n",
    "        print(f\"Chunk {i+1}:\\n{chunk.page_content}\\n{'-'*30}\")\n",
    "\n",
    "    # 3) Embeddings + Chroma\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vectordb = Chroma(\n",
    "        collection_name=\"nestle_hr_collection\",\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=str(CHROMA_DIR),\n",
    "    )\n",
    "    # Add & persist\n",
    "    vectordb.add_documents(chunks)\n",
    "    vectordb.persist()\n",
    "\n",
    "    # 4) Return retriever\n",
    "    return vectordb.as_retriever(search_kwargs={\"k\": SEARCH_K})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9f69535",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SYSTEM_PROMPT = '''You are a helpful assistant for Nestlé HR documents.\n",
    "Answer strictly and only from the provided context.\n",
    "If the answer is not in the context, reply: \"I could not find that in the HR documents.\"\n",
    "Be concise, cite page numbers when available, and include bullet points where helpful.\n",
    "'''\n",
    "\n",
    "QA_TEMPLATE = '''\n",
    "{system_prompt}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Guidelines:\n",
    "- If multiple policies conflict, state that and summarize both.\n",
    "- Prefer the latest policy if effective dates are shown.\n",
    "- Include page numbers from the source when available (e.g., \"Policy PDF p. 12\").\n",
    "- Keep answers factual and free of speculation.\n",
    "\n",
    "Answer:\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=QA_TEMPLATE,\n",
    "    input_variables=[\"system_prompt\", \"context\", \"question\"]\n",
    ")\n",
    "\n",
    "def make_qa_chain(retriever):\n",
    "    llm = ChatOpenAI(model=OPENAI_CHAT_MODEL, temperature=0)\n",
    "    # LangChain RetrievalQA \"stuff\" chain_type will stuff top-k docs into the prompt\n",
    "    chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": prompt, \"verbose\": False},\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "    return chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f372d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/86/4xbq_3f56559z4xwv7wg6ddh0000gn/T/ipykernel_36582/610584214.py:229: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"Chat with Nestlé HR Documents\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/86/4xbq_3f56559z4xwv7wg6ddh0000gn/T/ipykernel_36582/610584214.py:196: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  res = qa_chain({\"query\": message})\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Nestle HR Reports Chatbot\n",
    "-------------------------\n",
    "End-to-end app:\n",
    "- Upload PDFs (Nestlé HR policies/reports)\n",
    "- Build Chroma vector DB with OpenAI embeddings\n",
    "- Ask questions via a RetrievalQA chain (RAG)\n",
    "- Gradio UI\n",
    "\n",
    "Setup:\n",
    "    pip install -U langchain langchain-community langchain-openai chromadb pypdf gradio python-dotenv\n",
    "\n",
    "Environment:\n",
    "    Create .env with:\n",
    "        OPENAI_API_KEY=\"sk-...\"\n",
    "    (Optional) tweak:\n",
    "        OPENAI_CHAT_MODEL=\"gpt-3.5-turbo\"\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain core bits\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# UI\n",
    "import gradio as gr\n",
    "\n",
    "\n",
    "# ========= 0) Config & Environment =========\n",
    "load_dotenv()  # loads OPENAI_API_KEY from .env if present\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:\n",
    "    raise RuntimeError(\n",
    "        \"OPENAI_API_KEY not set. Create a .env with OPENAI_API_KEY or export it in your shell.\"\n",
    "    )\n",
    "\n",
    "# Model (assignment requests GPT-3.5 Turbo)\n",
    "OPENAI_CHAT_MODEL = os.getenv(\"OPENAI_CHAT_MODEL\", \"gpt-3.5-turbo\")\n",
    "\n",
    "# Chroma persistence\n",
    "CHROMA_DIR = Path(\"./chroma_nestle_hr\")\n",
    "CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Chunking\n",
    "CHUNK_SIZE = 1200\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "# Retrieval\n",
    "SEARCH_K = 5\n",
    "\n",
    "\n",
    "# ========= 1) Helpers: load & split PDFs =========\n",
    "def load_and_split_pdfs(pdf_paths):\n",
    "    \"\"\"\n",
    "    Load PDFs into LangChain Documents, then split into overlapping chunks.\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    for p in pdf_paths:\n",
    "        loader = PyPDFLoader(str(p))\n",
    "        docs.extend(loader.load())\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "# ========= 2) Build Chroma Vector DB with OpenAI embeddings =========\n",
    "def build_vector_store(chunks, persist_dir: Path, reset_db: bool = False):\n",
    "    \"\"\"\n",
    "    Create (or open) a Chroma store and add chunk embeddings.\n",
    "    \"\"\"\n",
    "    if reset_db and persist_dir.exists():\n",
    "        shutil.rmtree(persist_dir)\n",
    "        persist_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # OpenAI embeddings (use a cost-efficient default)\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "    vectordb = Chroma(\n",
    "        collection_name=\"nestle_hr_collection\",\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=str(persist_dir),\n",
    "    )\n",
    "\n",
    "    # Add documents in small batches (gentler on rate limits)\n",
    "    batch_size = 64\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i : i + batch_size]\n",
    "        vectordb.add_documents(batch)\n",
    "\n",
    "    vectordb.persist()\n",
    "    return vectordb\n",
    "\n",
    "\n",
    "# ========= 3) Prompt + RetrievalQA chain =========\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a helpful assistant for Nestlé HR documents. \"\n",
    "    \"Answer strictly and only from the provided context. \"\n",
    "    \"If the answer is not in the context, reply: 'I could not find that in the HR documents.' \"\n",
    "    \"Be concise, cite page numbers when available, and use bullet points if helpful.\"\n",
    ")\n",
    "\n",
    "QA_TEMPLATE = \"\"\"{system_prompt}\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Guidelines:\n",
    "- If multiple policies conflict, state that and summarize both.\n",
    "- Prefer the latest policy if effective dates are shown.\n",
    "- Include page numbers from the source when available (e.g., \"Policy PDF p. 12\").\n",
    "- Keep answers factual and free of speculation.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=QA_TEMPLATE,\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    partial_variables={\"system_prompt\": SYSTEM_PROMPT},\n",
    ")\n",
    "\n",
    "\n",
    "def make_qa_chain(vectordb, k=SEARCH_K):\n",
    "    retriever = vectordb.as_retriever(search_kwargs={\"k\": k})\n",
    "    llm = ChatOpenAI(model=OPENAI_CHAT_MODEL, temperature=0)\n",
    "    chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": prompt},\n",
    "        return_source_documents=True,\n",
    "    )\n",
    "    return chain\n",
    "\n",
    "\n",
    "# ========= 4) Gradio UI callbacks =========\n",
    "def _extract_paths(files):\n",
    "    \"\"\"\n",
    "    Convert gr.Files input to a list of Path objects.\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "    for f in files or []:\n",
    "        p = getattr(f, \"name\", None) or getattr(f, \"path\", None)\n",
    "        if not p:\n",
    "            raise ValueError(\"Could not resolve a filesystem path for an uploaded file.\")\n",
    "        paths.append(Path(p))\n",
    "    return paths\n",
    "\n",
    "\n",
    "def build_index_ui(files, reset_db):\n",
    "    \"\"\"\n",
    "    Build the vector index from uploaded PDFs.\n",
    "    Returns (status_text, qa_chain or None).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pdf_paths = _extract_paths(files)\n",
    "        if not pdf_paths:\n",
    "            return \"Please upload at least one PDF.\", None\n",
    "\n",
    "        chunks = load_and_split_pdfs(pdf_paths)\n",
    "        if not chunks:\n",
    "            return \"No text extracted from PDFs. Are they scanned images with no OCR?\", None\n",
    "\n",
    "        vectordb = build_vector_store(chunks, CHROMA_DIR, reset_db=reset_db)\n",
    "        qa_chain = make_qa_chain(vectordb)\n",
    "        return f\" Index built with {len(pdf_paths)} file(s) and {len(chunks)} chunks. You can start asking questions.\", qa_chain\n",
    "    except Exception as e:\n",
    "        tb = traceback.format_exc(limit=3)\n",
    "        return f\" Failed to build index:\\n{e}\\n\\n{tb}\", None\n",
    "\n",
    "\n",
    "def answer_question(message, history, qa_chain):\n",
    "    \"\"\"\n",
    "    Run RetrievalQA for a user question and format sources.\n",
    "    \"\"\"\n",
    "    if qa_chain is None:\n",
    "        return \"Please build the index first by uploading PDFs and clicking 'Build Index'.\"\n",
    "\n",
    "    res = qa_chain({\"query\": message})\n",
    "    answer = res[\"result\"]\n",
    "\n",
    "    # Append sources (filename & page)\n",
    "    try:\n",
    "        src_lines = []\n",
    "        for d in res.get(\"source_documents\", []):\n",
    "            src = d.metadata.get(\"source\", \"PDF\")\n",
    "            page = d.metadata.get(\"page\")\n",
    "            src_lines.append(f\"- {Path(src).name}, page {page + 1 if page is not None else '?'}\")\n",
    "        if src_lines:\n",
    "            answer += \"\\n\\n**Sources:**\\n\" + \"\\n\".join(src_lines)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return answer\n",
    "\n",
    "\n",
    "# ========= 5) Build Gradio app =========\n",
    "def make_app():\n",
    "    with gr.Blocks(title=\"Nestlé HR Chatbot\") as demo:\n",
    "        gr.Markdown(\"# Nestlé HR Reports Chatbot\")\n",
    "        gr.Markdown(\"Upload Nestlé HR policy/reports (PDF), build the index, and ask questions.\")\n",
    "\n",
    "        with gr.Row():\n",
    "            files = gr.Files(file_types=[\".pdf\"], label=\"Upload Nestlé HR PDFs\")\n",
    "        with gr.Row():\n",
    "            reset = gr.Checkbox(\n",
    "                label=\"Rebuild index from scratch (clears existing Chroma store)\", value=False\n",
    "            )\n",
    "            build = gr.Button(\"Build Index\", variant=\"primary\")\n",
    "        status = gr.Markdown(\"\")\n",
    "\n",
    "        chatbot = gr.Chatbot(label=\"Chat with Nestlé HR Documents\")\n",
    "        question = gr.Textbox(label=\"Ask a question\")\n",
    "        qa_state = gr.State(value=None)  # persist the RetrievalQA chain between events\n",
    "\n",
    "        def add_user_msg(user_msg, chat_history):\n",
    "            if not user_msg:\n",
    "                return \"\", chat_history\n",
    "            return \"\", chat_history + [[user_msg, None]]\n",
    "\n",
    "        def add_bot_msg(chat_history, qa_chain):\n",
    "            user_msg = chat_history[-1][0]\n",
    "            reply = answer_question(user_msg, chat_history, qa_chain)\n",
    "            chat_history[-1][1] = reply\n",
    "            return chat_history\n",
    "\n",
    "        # Build index -> status text + store qa_chain in state\n",
    "        build.click(build_index_ui, inputs=[files, reset], outputs=[status, qa_state])\n",
    "\n",
    "        # Chat flow\n",
    "        question.submit(\n",
    "            add_user_msg, inputs=[question, chatbot], outputs=[question, chatbot], queue=False\n",
    "        ).then(\n",
    "            add_bot_msg, inputs=[chatbot, qa_state], outputs=[chatbot]\n",
    "        )\n",
    "\n",
    "    return demo\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = make_app()\n",
    "    # Local-only:\n",
    "    app.launch(server_name=\"127.0.0.1\", server_port=7860, share=False)\n",
    "    # Or LAN access (other devices on your Wi-Fi):\n",
    "    # app.launch(server_name=\"0.0.0.0\", server_port=7860, share=False)\n",
    "    # Or auto-open browser:\n",
    "    # app.launch(inbrowser=True)\n",
    "    # Or temporary public link:\n",
    "    # app.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e13ecdc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "Policy\n",
      "Mandatory\n",
      "September  2012\n",
      "The Nestlé  \n",
      "Human Resources Policy\n",
      "------------------------------\n",
      "Chunk 2:\n",
      "Policy\n",
      "Mandatory\n",
      "September \n",
      " 20\n",
      "12\n",
      "Issuing departement\n",
      "Hum\n",
      "an Resources\n",
      "Target audience \n",
      "All\n",
      " employees\n",
      "Approver\n",
      "Executive Board, Nestlé S.A.\n",
      "Repository\n",
      "All Nestlé Principles and Policies, Standards and  \n",
      "Guidelines can be found in the Centre online repository at:  \n",
      "http://intranet.nestle.com/nestledocs\n",
      "Copyright\n",
      " and confidentiality\n",
      "Al\n",
      "l rights belong to Nestec Ltd., Vevey, Switzerland.\n",
      "© 2012, Nestec Ltd.\n",
      "Design\n",
      "Nestec Ltd., Corporate Identity & Design,  \n",
      "Vevey, Switzerland\n",
      "Production\n",
      "------------------------------\n",
      "Chunk 3:\n",
      "Vevey, Switzerland\n",
      "Production\n",
      "brain’print GmbH, Switzerland\n",
      "Paper\n",
      "This report is printed on BVS, a paper produced  \n",
      "from well-managed forests and other controlled sources  \n",
      "certified by the Forest Stewardship Council (FSC).\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/86/4xbq_3f56559z4xwv7wg6ddh0000gn/T/ipykernel_36582/268736056.py:27: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectordb = Chroma(\n",
      "/var/folders/86/4xbq_3f56559z4xwv7wg6ddh0000gn/T/ipykernel_36582/268736056.py:34: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/deepthi/.pyenv/versions/voicebot-env/lib/python3.10/site-packages/gradio/queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/Users/deepthi/.pyenv/versions/voicebot-env/lib/python3.10/site-packages/gradio/route_utils.py\", line 350, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/deepthi/.pyenv/versions/voicebot-env/lib/python3.10/site-packages/gradio/blocks.py\", line 2250, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/deepthi/.pyenv/versions/voicebot-env/lib/python3.10/site-packages/gradio/blocks.py\", line 1757, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/Users/deepthi/.pyenv/versions/voicebot-env/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/Users/deepthi/.pyenv/versions/voicebot-env/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2476, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/deepthi/.pyenv/versions/voicebot-env/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Users/deepthi/.pyenv/versions/voicebot-env/lib/python3.10/site-packages/gradio/utils.py\", line 917, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/86/4xbq_3f56559z4xwv7wg6ddh0000gn/T/ipykernel_36582/3891487513.py\", line 18, in build_index_ui\n",
      "    state[\"qa_chain\"] = make_qa_chain(retriever)\n",
      "  File \"/var/folders/86/4xbq_3f56559z4xwv7wg6ddh0000gn/T/ipykernel_36582/545315937.py\", line 30, in make_qa_chain\n",
      "    llm = ChatOpenAI(model=OPENAI_CHAT_MODEL, temperature=0)\n",
      "NameError: name 'OPENAI_CHAT_MODEL' is not defined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "Policy\n",
      "Mandatory\n",
      "September  2012\n",
      "The Nestlé  \n",
      "Human Resources Policy\n",
      "------------------------------\n",
      "Chunk 2:\n",
      "Policy\n",
      "Mandatory\n",
      "September \n",
      " 20\n",
      "12\n",
      "Issuing departement\n",
      "Hum\n",
      "an Resources\n",
      "Target audience \n",
      "All\n",
      " employees\n",
      "Approver\n",
      "Executive Board, Nestlé S.A.\n",
      "Repository\n",
      "All Nestlé Principles and Policies, Standards and  \n",
      "Guidelines can be found in the Centre online repository at:  \n",
      "http://intranet.nestle.com/nestledocs\n",
      "Copyright\n",
      " and confidentiality\n",
      "Al\n",
      "l rights belong to Nestec Ltd., Vevey, Switzerland.\n",
      "© 2012, Nestec Ltd.\n",
      "Design\n",
      "Nestec Ltd., Corporate Identity & Design,  \n",
      "Vevey, Switzerland\n",
      "Production\n",
      "------------------------------\n",
      "Chunk 3:\n",
      "Vevey, Switzerland\n",
      "Production\n",
      "brain’print GmbH, Switzerland\n",
      "Paper\n",
      "This report is printed on BVS, a paper produced  \n",
      "from well-managed forests and other controlled sources  \n",
      "certified by the Forest Stewardship Council (FSC).\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/deepthi/.pyenv/versions/voicebot-env/lib/python3.10/site-packages/gradio/queueing.py\", line 626, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"/Users/deepthi/.pyenv/versions/voicebot-env/lib/python3.10/site-packages/gradio/route_utils.py\", line 350, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"/Users/deepthi/.pyenv/versions/voicebot-env/lib/python3.10/site-packages/gradio/blocks.py\", line 2250, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"/Users/deepthi/.pyenv/versions/voicebot-env/lib/python3.10/site-packages/gradio/blocks.py\", line 1757, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "  File \"/Users/deepthi/.pyenv/versions/voicebot-env/lib/python3.10/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"/Users/deepthi/.pyenv/versions/voicebot-env/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 2476, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/deepthi/.pyenv/versions/voicebot-env/lib/python3.10/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/Users/deepthi/.pyenv/versions/voicebot-env/lib/python3.10/site-packages/gradio/utils.py\", line 917, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "  File \"/var/folders/86/4xbq_3f56559z4xwv7wg6ddh0000gn/T/ipykernel_36582/3891487513.py\", line 16, in build_index_ui\n",
      "    retriever = build_knowledge_base(pdf_paths, reset_db=reset_db)\n",
      "  File \"/var/folders/86/4xbq_3f56559z4xwv7wg6ddh0000gn/T/ipykernel_36582/268736056.py\", line 33, in build_knowledge_base\n",
      "    vectordb.add_documents(chunks)\n",
      "  File \"/Users/deepthi/.pyenv/versions/voicebot-env/lib/python3.10/site-packages/langchain_core/vectorstores/base.py\", line 288, in add_documents\n",
      "    return self.add_texts(texts, metadatas, **kwargs)\n",
      "  File \"/Users/deepthi/.pyenv/versions/voicebot-env/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py\", line 299, in add_texts\n",
      "    self._collection.upsert(\n",
      "  File \"/Users/deepthi/.pyenv/versions/voicebot-env/lib/python3.10/site-packages/chromadb/api/models/Collection.py\", line 374, in upsert\n",
      "    self._client._upsert(\n",
      "  File \"/Users/deepthi/.pyenv/versions/voicebot-env/lib/python3.10/site-packages/chromadb/api/rust.py\", line 464, in _upsert\n",
      "    return self.bindings.upsert(\n",
      "chromadb.errors.InternalError: Query error: Database error: error returned from database: (code: 1032) attempt to write a readonly database\n"
     ]
    }
   ],
   "source": [
    "# To run locally, uncomment:\n",
    "demo.launch(server_name=\"0.0.0.0\", server_port=7860, share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
